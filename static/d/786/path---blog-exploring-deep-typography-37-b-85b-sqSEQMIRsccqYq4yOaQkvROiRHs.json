{"data":{"markdownRemark":{"html":"<h2>i. Motivation</h2>\n<p>My brother writes his <a href=\"https://www.hohyeonmoon.com/\">developer's blog</a> in Korean. But often, he writes certain terminologies in English.\nAn important part of a blog aesthetic is the font. While there are few hundreds of free English fonts, there aren't that many available in Korean.\nSo he had to spend hours finding the right Korean and English fonts that were available for free, but also look good enough together. </p>\n<p>Creating a new font is an absolute pita. Even more so for Asian characters that have more character numbers.\nKorean has 40 characters, and a Korean <em>word</em> is a combination of at least 2 characters.\nWith <a href=\"https://type.method.ac/\">kerning</a> included, the font design process requires hard manual work on each pixel. </p>\n<p>What if font styles could be transferred from one language to another? </p>\n<p>In search of <strong>a neural network that can properly create new fonts</strong>, this series will have three different posts: </p>\n<br/>\n<p><strong>1.  Naive approach</strong>  exploring font space with deep learning methods such as VGG16 and T-SNE. </p>\n<p><strong>2.  Advanced approach</strong>  exploring font space with deep learning methods such as autoencoder, VAE, and GAN.</p>\n<p><strong>3.  Future works</strong>  personal projects.</p>\n<br/>\n<p><strong>**</strong> <em>This post (post 1) covers part 1. Posts 2, 3 are soon to be uploaded</em>.</p>\n<h2>Exploring font space using CNN &#x26; T-SNE</h2>\n<p>There have been two interesting projects that have attempted to :</p>\n<p><strong>a)</strong> visualize distribution of similar font types in clusters using VGG16 &#x26; T-SNE</p>\n<p><strong>b)</strong> create new fonts by training using CNN &#x26; T-SNE  </p>\n<h2>ii. Visualizing font space</h2>\n<p>Two separate projects - one by Kevin Ho, another by Jack Qiao - have wondefully visualized various fonts on a low dimensional (2D or 3D) space map.\nBoth projects leveraged VGG16 to extract feature vectors off font images. Then, both ran the extracted feature embeddings through T-SNE.</p>\n<br/>\n<p><em>disclosure: this part of the post has been inspired by Kevin Ho's <a href=\"https://medium.com/ideo-stories/organizing-the-world-of-fonts-with-ai-7d9e49ff2b25\">original post</a>\n&#x26; Jack Qiao's <a href=\"https://github.com/Jack000/fontjoy\">github repo</a>.</em></p>\n<h3>1) implementation steps</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 780px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/80b4d2dda823603db540765589c3d7bf/cc43b/post04-ideo-implementation.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 74.97500000000001%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe9UNIP/xAAVEAEBAAAAAAAAAAAAAAAAAAAgQf/aAAgBAQABBQKn/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAIP/aAAgBAQAGPwJf/8QAGRAAAgMBAAAAAAAAAAAAAAAAAREQICFR/9oACAEBAAE/ISSgWdotcf/aAAwDAQACAAMAAAAQcM//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEAAgIDAAAAAAAAAAAAAAABABEhQRAxUf/aAAgBAQABPxABGoz4l3yvEuCq1C6L73P/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"ideo-implementation\"\n        title=\"ideo-implementation\"\n        src=\"/static/80b4d2dda823603db540765589c3d7bf/751b3/post04-ideo-implementation.jpg\"\n        srcset=\"/static/80b4d2dda823603db540765589c3d7bf/47425/post04-ideo-implementation.jpg 195w,\n/static/80b4d2dda823603db540765589c3d7bf/53d89/post04-ideo-implementation.jpg 390w,\n/static/80b4d2dda823603db540765589c3d7bf/751b3/post04-ideo-implementation.jpg 780w,\n/static/80b4d2dda823603db540765589c3d7bf/b2d4e/post04-ideo-implementation.jpg 1170w,\n/static/80b4d2dda823603db540765589c3d7bf/86a84/post04-ideo-implementation.jpg 1560w,\n/static/80b4d2dda823603db540765589c3d7bf/cc43b/post04-ideo-implementation.jpg 4000w\"\n        sizes=\"(max-width: 780px) 100vw, 780px\"\n      />\n  </a>\n    </span></p>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<blockquote>\n<p><a href=\"https://medium.com/ideo-stories/organizing-the-world-of-fonts-with-ai-7d9e49ff2b25\">figure source</a></p>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n</blockquote>\n<ol>\n<li>Dataset:  Create a 'Handgloves' image for each font. (about 800 fonts in total, so that would make about 800 different 'Handgloves' images)</li>\n<li>Extract feature vector:  For each font image, run through VGG16 Convolutional Neural Network to get a feature embedding. </li>\n<li>Visualize:  Run all feature vectors through T-SNE to visualize their distributions over a 2-D plane. </li>\n</ol>\n<p>Visualization results can be viewed here:</p>\n<p><a href=\"http://fontmap.ideo.com/\">Kevin Ho (IDEO)</a></p>\n<p><a href=\"https://fontjoy.com/projector/\">Jack Qiao</a></p>\n<h3>2) applications</h3>\n<p>Both projects are applied to a design aid to help designers make easier font choice. Designers can look up <a href=\"http://fontmap.ideo.com/\">IDEO's font map</a>, or Jack Qiao's web app <a href=\"http://fontjoy.com\">fontjoy.com</a> to find out which font pairings would go well together.</p>\n<h2>iii. Creating new fonts</h2>\n<p>A project by Patrick Gadd attempts to create proportional fonts with a neural network.\nWhat is unique about this project is that it creates proportioanl fonts. Proportional font is a font that also has <a href=\"https://en.wikipedia.org/wiki/Kerning\">kernings</a> included in its design. </p>\n<br/>\n<p><em>disclosure: This part of the post has been inspired by <a href=\"https://patrickgadd.github.io/feel-the-kern/\">this blog post</a> &#x26; corresponding <a href=\"https://github.com/patrickgadd/feel-the-kern\">github repo</a>.</em></p>\n<h3>1) implementation steps</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 780px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/af73f073d09bcb748676ba4729d75d81/8e427/post04-feel-the-kern-arch.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 63.71158392434988%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAABh0lEQVQ4y6VT2W7jMAzM/78FSJ4D5LsWcdut16ccK7ZuybMi3aRJUCxaLG2CtEyNeIw2y7KAJIQAYw188LDOIqaIVRZcY74jm2uwUjOqskbfDPj9UmIaFZIHUlz/fxd08xAc8msTrHaYJw07e0T3Q0AO5HdBDImzmqWG04HB/yvDtCQorXAqThCDQPro4496eL/Bew8hBhSnAuV7Ca30AyDZe71fexrKWjaDBoe6qTCr6eHkr7J8PoD0liE9VkaYHmhfJOY20+gcc9mJlWJijHDOwVrLa/8s2RgNKWbILttWZatwESrz09+CpZSZXgpaa1wulxt/mcPG8CGbh9Q/6g551MMo0IsOb2+vKIoi91ZgGAb0fY9xHNknoOPxiP1+j91uh7quP4nNwGlVZwPGs8R5kDycrus4u6Zp2Cdt25azPRwO2G63DFhV1ZohXTOtDOQfB1k69L80W/meiW3W60dCZRITqJfXkp+HtALGyBTxzvMGstR8Zx1/k0/9IX+aJlYq9yva/AWPFvWooHgWqgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"feel-the-kern-arch\"\n        title=\"feel-the-kern-arch\"\n        src=\"/static/af73f073d09bcb748676ba4729d75d81/164b6/post04-feel-the-kern-arch.png\"\n        srcset=\"/static/af73f073d09bcb748676ba4729d75d81/b7004/post04-feel-the-kern-arch.png 195w,\n/static/af73f073d09bcb748676ba4729d75d81/14f69/post04-feel-the-kern-arch.png 390w,\n/static/af73f073d09bcb748676ba4729d75d81/164b6/post04-feel-the-kern-arch.png 780w,\n/static/af73f073d09bcb748676ba4729d75d81/df67e/post04-feel-the-kern-arch.png 1170w,\n/static/af73f073d09bcb748676ba4729d75d81/884f2/post04-feel-the-kern-arch.png 1560w,\n/static/af73f073d09bcb748676ba4729d75d81/8e427/post04-feel-the-kern-arch.png 1692w\"\n        sizes=\"(max-width: 780px) 100vw, 780px\"\n      />\n  </a>\n    </span></p>\n<ol>\n<li>\n<p>Encode novel font styles using t-SNE:  Map the fonts you have to z-dimensional vectors expressing their \"style\".</p>\n<ul>\n<li>Dimensionality space <strong>z</strong> is a continuous space where we can interpolate between the original fonts.</li>\n<li>Thus, this will allow us to create a continuum of fonts which are novel combinations of the original fonts, like below: (<a href=\"https://patrickgadd.github.io/feel-the-kern/\">figure source</a>)\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 780px;\"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 48.42931937172775%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAABCElEQVQoz42S226DMBBE+f9/IpXCC5cGBIVS4YC4iBcgClVSgQJmapyUJjWQWlqvJa+PZ+yVMDOGYZhdL9XcD0ks5PN04NJTvHkf2O1eoes6VFVFEAQT9C9Ymrt5uGXKomNr13XxstlAlmUemqYtKpVEdb9F547CdN+hbLewbRuKoiDP80V1D8DhKmsq+mJWD6czwj2BZVnwfZ9HURSglD63fG91zO1o1fM4zDRNGIaBLMsE0BPL183PS48girBnj+84DgghqKqKK1tTJyj8+dWm65HEMdI0RcTASZKgLMtV0CLweDyxtiAIwxAxg44t0rbtv/tTAI6W6rpmFg+3XKFpmgd1awq/Ac6SB80WzTP4AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"font-continuum\"\n        title=\"font-continuum\"\n        src=\"/static/31bc072a21ba1a0433be2bffb1a15e14/164b6/post04-font-continuum.png\"\n        srcset=\"/static/31bc072a21ba1a0433be2bffb1a15e14/b7004/post04-font-continuum.png 195w,\n/static/31bc072a21ba1a0433be2bffb1a15e14/14f69/post04-font-continuum.png 390w,\n/static/31bc072a21ba1a0433be2bffb1a15e14/164b6/post04-font-continuum.png 780w,\n/static/31bc072a21ba1a0433be2bffb1a15e14/df67e/post04-font-continuum.png 1170w,\n/static/31bc072a21ba1a0433be2bffb1a15e14/a6f9d/post04-font-continuum.png 1528w\"\n        sizes=\"(max-width: 780px) 100vw, 780px\"\n      />\n    </span></li>\n<li>\n<p>This project chose z = 10. This would create a 10-dimensional style vector.</p>\n<ul>\n<li>The 10-dimensional style-vector is simply the point in the space Z which t-SNE told me corresponds to the given font.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>Encode bigram:  create a one-hot encoding of bigrams.</p>\n<ul>\n<li>A bigram of an alphabet that writes <code>ET</code>would look like something like this:</li>\n</ul>\n<p><code>00001000 00000000 00000000 000</code>\n<br/>\n<code>00000000 00000000 00010000 000</code></p>\n<ul>\n<li>Why encode it as one-hot vector?  Because we want each alphabet encoding to have no correlation to another.</li>\n</ul>\n</li>\n<li>\n<p>Concat ( style encoding + bigram encoding ) into a single vector &#x26; Train a CNN (convolutional neural network) that takes a concatenated input vector, and outputs the corresponding bigram feature vector.</p>\n</li>\n<li>\n<p>Overlap bigrams to complete full words:  Generate bigrams of new styles, and use simulated annealing to overlap the bigrams to full words.</p>\n<ul>\n<li>overlapping process can be seen here</li>\n</ul>\n</li>\n</ol>\n<h2>iv. Possible improvements</h2>\n<!-- \nseries into 3 parts - intro \n1. VGG\n2. feel the kern 무슨 nn?\n3. auto encoder\n4. vae, gan \n5. future 1 :  style transfer :  english fonts to korean  using cycle gan??? \n\tthen what is cycle gan ??? where is it used?\n\twhat other ways are there to trasfer style?\n6. future 2 : new fonts dataset -- only containing fonts with legit characters \n\t- google fonts\n\t- behance free fonts \n\t- ==> train with these fonts and see if the nn creates fonts that look like ones that arent free (ex. apercu)\n7. future 3:  \n-->","excerpt":"i. MotivationMy brother writes his developer's blog in Korean. But often, he writes certain terminologies in English. \nAn important part of a blog aesthetic is…","frontmatter":{"date":"03 January, 2020","path":"/blog/exploring-deep-typography","title":"Exploring Deep Font"},"fields":{"readingTime":{"text":"5 min read"}}}},"pageContext":{"isCreatedByStatefulCreatePages":false}}